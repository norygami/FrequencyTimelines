{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: e:\\Frequency_Timelines\\in\\tp3p1.csv\n",
      "Converting cumulative frequencies to DataFrame\n",
      "Table saved to e:\\Frequency_Timelines\\out/csv\\klimaskeptiker_klimawandelskeptiker_klimaleugner_klimawandelleugner_Occurrences.tsv\n"
     ]
    }
   ],
   "source": [
    "# Cumulative (Table)\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Input\n",
    "corpus = 'tp3p1.csv'\n",
    "\n",
    "# Terms to visualize\n",
    "specific_terms = [\n",
    "                #'artensterben', \n",
    "                #'bienensterben', \n",
    "                #'insektensterben',\n",
    "                #'waldsterben',\n",
    "                #'waldschÃ¤den',\n",
    "                #'baumsterben',\n",
    "                #'fichtensterben',\n",
    "                #'tannensterben',\n",
    "                #'klimawandel',\n",
    "                #'klimawandel',\n",
    "                #'umweltschutz',\n",
    "                #'nachhaltigkeit'\n",
    "                #'skeptiker',\n",
    "                'klimaskeptiker',\n",
    "                'klimawandelskeptiker',\n",
    "                #'leugner',\n",
    "                'klimaleugner',\n",
    "                'klimawandelleugner',\n",
    "                ]\n",
    "\n",
    "# Filters to apply\n",
    "filters = {\n",
    "    'text_text_type': ['zeitung', 'wochenzeitung'],\n",
    "    # 'another_column': ['value1', 'value2'],\n",
    "}\n",
    "\n",
    "# Paths\n",
    "cwd = os.getcwd()\n",
    "directory_path = os.path.join(cwd, 'in')\n",
    "outputDir = os.path.join(cwd, 'out/csv')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', str(text).lower())\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Apply filters to the chunk\n",
    "def apply_filters(chunk, filters):\n",
    "    for column, values in filters.items():\n",
    "        if column in chunk.columns:\n",
    "            chunk = chunk[chunk[column].isin(values)]\n",
    "    return chunk\n",
    "\n",
    "# Calculate absolute and relative frequencies for a chunk\n",
    "def calculate_frequencies(chunk, specific_terms):\n",
    "    chunk['clean_text'] = chunk['text_content'].apply(preprocess_text)\n",
    "    chunk['year'] = chunk['text_date'].str[:4].astype(int)\n",
    "    term_frequencies = {}\n",
    "    total_word_counts = {}\n",
    "    for year in chunk['year'].unique():\n",
    "        year_text = ' '.join(chunk[chunk['year'] == year]['clean_text'])\n",
    "        word_counts = Counter(year_text.split())\n",
    "        total_words = sum(word_counts.values())\n",
    "        total_word_counts[year] = total_words\n",
    "        frequencies = {term: word_counts.get(term, 0) for term in specific_terms}\n",
    "        if year not in term_frequencies:\n",
    "            term_frequencies[year] = Counter()\n",
    "        term_frequencies[year].update(frequencies)\n",
    "    return term_frequencies, total_word_counts\n",
    "\n",
    "# Prepare data for saving\n",
    "def prepare_data_for_saving(term_frequencies, total_word_counts):\n",
    "    plot_data = []\n",
    "    for year, frequencies in term_frequencies.items():\n",
    "        total_words = total_word_counts[year]\n",
    "        for term, frequency in frequencies.items():\n",
    "            relative_frequency = (frequency / total_words) * 1e6 if total_words > 0 else 0\n",
    "            plot_data.append({\n",
    "                'year': year, \n",
    "                'term': term, \n",
    "                'absolute_frequency': frequency, \n",
    "                'relative_frequency_per_million': relative_frequency\n",
    "            })\n",
    "    return pd.DataFrame(plot_data)\n",
    "\n",
    "# Outfile named after specific_terms\n",
    "terms_concatenated = '_'.join(specific_terms)\n",
    "\n",
    "# Cumulative term frequencies and total word counts across the file\n",
    "cumulative_frequencies = {}\n",
    "cumulative_word_counts = {}\n",
    "\n",
    "# Chunk size for reading the CSV file\n",
    "chunk_size = 100000\n",
    "\n",
    "file_path = os.path.join(directory_path, corpus)\n",
    "print(f\"Input: {file_path}\")\n",
    "\n",
    "# Read the file in chunks\n",
    "for chunk in pd.read_csv(file_path, delimiter='\\t', chunksize=chunk_size):\n",
    "    # Apply filters to the chunk\n",
    "    chunk = apply_filters(chunk, filters)\n",
    "    \n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    term_frequencies, total_word_counts = calculate_frequencies(chunk, specific_terms)\n",
    "    \n",
    "    # Add to cumulative frequencies and word counts\n",
    "    for year, frequencies in term_frequencies.items():\n",
    "        if year not in cumulative_frequencies:\n",
    "            cumulative_frequencies[year] = Counter()\n",
    "            cumulative_word_counts[year] = 0\n",
    "        cumulative_frequencies[year].update(frequencies)\n",
    "        cumulative_word_counts[year] += total_word_counts[year]\n",
    "\n",
    "# Convert cumulative frequencies to DataFrame\n",
    "print(\"Converting cumulative frequencies to DataFrame\")\n",
    "cumulative_plot_df = prepare_data_for_saving(cumulative_frequencies, cumulative_word_counts)\n",
    "cumulative_plot_df['year'] = cumulative_plot_df['year'].astype(int)\n",
    "cumulative_plot_df = cumulative_plot_df.sort_values(by=['year'])\n",
    "\n",
    "# Save the results into table (TSV)\n",
    "occurrences_output_filename = f'{terms_concatenated}_Occurrences.csv'\n",
    "cumulative_plot_df.to_csv(os.path.join(outputDir, occurrences_output_filename), sep='\\t', index=False)\n",
    "print(f\"Table saved to {os.path.join(outputDir, occurrences_output_filename)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: klimaskeptiker_klimawandelskeptiker_klimaleugner_klimawandelleugner_Occurrences.tsv\n",
      "Absolute Frequency Visualization saved to e:\\Frequency_Timelines\\out/html/klimaskeptiker_klimawandelskeptiker_klimaleugner_klimawandelleugner_absolute_frequency_Cumulative.html\n",
      "Relative Frequency per Million Visualization saved to e:\\Frequency_Timelines\\out/html/klimaskeptiker_klimawandelskeptiker_klimaleugner_klimawandelleugner_relative_frequency_per_million_Cumulative.html\n",
      "Processing file: klimawandel_umweltschutz_nachhaltigkeit_Occurrences.tsv\n",
      "Absolute Frequency Visualization saved to e:\\Frequency_Timelines\\out/html/klimawandel_umweltschutz_nachhaltigkeit_absolute_frequency_Cumulative.html\n",
      "Relative Frequency per Million Visualization saved to e:\\Frequency_Timelines\\out/html/klimawandel_umweltschutz_nachhaltigkeit_relative_frequency_per_million_Cumulative.html\n",
      "Processing file: skeptiker_klimaskeptiker_klimawandelskeptiker_leugner_klimaleugner_klimawandelleugner_Occurrences.tsv\n",
      "Absolute Frequency Visualization saved to e:\\Frequency_Timelines\\out/html/skeptiker_klimaskeptiker_klimawandelskeptiker_leugner_klimaleugner_klimawandelleugner_absolute_frequency_Cumulative.html\n",
      "Relative Frequency per Million Visualization saved to e:\\Frequency_Timelines\\out/html/skeptiker_klimaskeptiker_klimawandelskeptiker_leugner_klimaleugner_klimawandelleugner_relative_frequency_per_million_Cumulative.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "cwd = os.getcwd()\n",
    "input_dir = os.path.join(cwd, 'out/csv')\n",
    "html_output_dir = os.path.join(cwd, 'out/html/')\n",
    "\n",
    "# Get the list of all files in the tsv_output_dir that match \"Occurrences.tsv\"\n",
    "files = [f for f in os.listdir(input_dir) if \"Occurrences.tsv\" in f]\n",
    "\n",
    "if files:\n",
    "    for occurrences_output_filename in files:\n",
    "        print(f\"Processing file: {occurrences_output_filename}\")\n",
    "        cumulative_plot_df = pd.read_csv(os.path.join(input_dir, occurrences_output_filename), sep='\\t')\n",
    "\n",
    "        # Restrict the data to the range 1990-2020\n",
    "        cumulative_plot_df = cumulative_plot_df[(cumulative_plot_df['year'] >= 1990) & (cumulative_plot_df['year'] <= 2020)]\n",
    "\n",
    "        # Use a Plotly built-in color scheme\n",
    "        color_sequence = px.colors.qualitative.D3 \n",
    "\n",
    "        # Create the plot with custom styling for both absolute and relative frequencies\n",
    "        for freq_type, y_label, y_column in [('absolute_frequency', 'Absolute Frequency', 'absolute_frequency'), \n",
    "                                             ('relative_frequency_per_million', 'Relative Frequency per Million', 'relative_frequency_per_million')]:\n",
    "            \n",
    "            labels = cumulative_plot_df['term'].unique()\n",
    "            fig = go.Figure()\n",
    "\n",
    "            # Trace & Marker dimentions\n",
    "            size = [8]\n",
    "            width = [2]\n",
    "\n",
    "            for i, term in enumerate(labels):\n",
    "                term_data = cumulative_plot_df[cumulative_plot_df['term'] == term]\n",
    "                x_data = term_data['year']\n",
    "                y_data = term_data[y_column]\n",
    "\n",
    "                # Add the line trace for the term, using the color sequence\n",
    "                fig.add_trace(go.Scatter(x=x_data, y=y_data, mode='lines',\n",
    "                    name=term.capitalize(),\n",
    "                    line=dict(color=color_sequence[i % len(color_sequence)], width=width[i % len(width)]),\n",
    "                    marker=dict(size=2),\n",
    "                    connectgaps=True,\n",
    "                ))\n",
    "\n",
    "                # Add markers for the endpoints\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=[x_data.iloc[0], x_data.iloc[-1]],\n",
    "                    y=[y_data.iloc[0], y_data.iloc[-1]],\n",
    "                    mode='markers',\n",
    "                    marker=dict(color=color_sequence[i % len(color_sequence)], size=size[i % len(size)])\n",
    "                ))\n",
    "\n",
    "            # Custom layout styling\n",
    "            fig.update_layout(\n",
    "                width=1100,\n",
    "                xaxis=dict(\n",
    "                    showline=True,\n",
    "                    showgrid=True,\n",
    "                    gridcolor='rgba(200, 200, 200, 0.3)',\n",
    "                    showticklabels=True,\n",
    "                    linecolor='rgb(204, 204, 204)',\n",
    "                    linewidth=2,\n",
    "                    ticks='outside',\n",
    "                    tickmode='array',  # Custom tick mode\n",
    "                    tickvals=[i for i in range(1990, 2021, 2)], \n",
    "                    tickfont=dict(\n",
    "                        family='Arial',\n",
    "                        size=12,\n",
    "                        color='rgb(82, 82, 82)',\n",
    "                    ),\n",
    "                ),\n",
    "                yaxis=dict(\n",
    "                    showgrid=True,\n",
    "                    gridcolor='rgba(200, 200, 200, 0.3)', \n",
    "                    zeroline=False,\n",
    "                    showline=False,\n",
    "                    showticklabels=True,\n",
    "                    tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)'),\n",
    "                ),\n",
    "                autosize=False,\n",
    "                margin=dict(\n",
    "                    autoexpand=False,\n",
    "                    l=100,\n",
    "                    r=200,\n",
    "                    t=110,\n",
    "                ),\n",
    "                plot_bgcolor='white',\n",
    "                showlegend=False  # Remove the legend\n",
    "            )\n",
    "\n",
    "            # Add annotations for the right-side labels, slightly moved to the right\n",
    "            annotations = []\n",
    "            for i, term in enumerate(labels):\n",
    "                term_data = cumulative_plot_df[cumulative_plot_df['term'] == term]\n",
    "                y_trace = term_data[y_column].values\n",
    "                # Labeling the right side of the plot (end of the line)\n",
    "                annotations.append(dict(xref='paper', x=1.01, y=y_trace[-1],  # Adjusted to move labels slightly to the right\n",
    "                                        xanchor='left', yanchor='middle',\n",
    "                                        text=term.capitalize(),\n",
    "                                        font=dict(family='Arial', size=16, color=color_sequence[i % len(color_sequence)]),\n",
    "                                        showarrow=False))\n",
    "\n",
    "            # Update layout with annotations\n",
    "            fig.update_layout(annotations=annotations)\n",
    "\n",
    "            # Save the plot as an HTML file\n",
    "            output_filename = occurrences_output_filename.replace('Occurrences.tsv', f'{freq_type}_Cumulative.html')\n",
    "            fig.write_html(file=os.path.join(html_output_dir, output_filename), include_plotlyjs=True)\n",
    "            print(f\"{y_label} Visualization saved to {os.path.join(html_output_dir, output_filename)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No 'Occurrences.tsv' files found in the specified directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
