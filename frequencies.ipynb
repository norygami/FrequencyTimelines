{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: e:\\Frequency_Timelines/in/tp3p2_sub.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Read the file in chunks\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39mchunk_size):\n\u001b[1;32m---> 81\u001b[0m     term_frequencies, total_word_counts \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecific_terms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# Add to cumulative frequencies and word counts\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m year, frequencies \u001b[38;5;129;01min\u001b[39;00m term_frequencies\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m, in \u001b[0;36mcalculate_frequencies\u001b[1;34m(chunk, specific_terms)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_frequencies\u001b[39m(chunk, specific_terms):\n\u001b[1;32m---> 36\u001b[0m     chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr[:\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# Extracting the year from the text_date column\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     term_frequencies \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Nouremz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4791\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nouremz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nouremz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Nouremz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nouremz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[0;32m     30\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(text)\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m---> 31\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32mc:\\Users\\Nouremz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\re\\__init__.py:178\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msearch(string)\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cumulative (Table)\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "corpus = 'tp3p2_sub.csv'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "directory_path = cwd + '/in/'\n",
    "outputDir = cwd + '/out/csv/'\n",
    "\n",
    "# List of specific terms you want to visualize\n",
    "specific_terms = [\n",
    "                #'artensterben', \n",
    "                #'bienensterben', \n",
    "                #'insektensterben',\n",
    "                #'waldsterben',\n",
    "                #'waldschäden',\n",
    "                #'baumsterben',\n",
    "                #'fichtensterben',\n",
    "                #'tannensterben',\n",
    "                #'klimawandel'\n",
    "                'Bildungsstandard',\n",
    "                'Inklusion'\n",
    "                ]\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', str(text).lower())\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Function to calculate absolute and relative frequencies for a chunk\n",
    "def calculate_frequencies(chunk, specific_terms):\n",
    "    chunk['clean_text'] = chunk['text_content'].apply(preprocess_text)\n",
    "    chunk['year'] = chunk['text_date'].str[:4].astype(int)  # Extracting the year from the text_date column\n",
    "    term_frequencies = {}\n",
    "    total_word_counts = {}\n",
    "    for year in chunk['year'].unique():\n",
    "        year_text = ' '.join(chunk[chunk['year'] == year]['clean_text'])\n",
    "        word_counts = Counter(year_text.split())\n",
    "        total_words = sum(word_counts.values())\n",
    "        total_word_counts[year] = total_words\n",
    "        frequencies = {term: word_counts.get(term, 0) for term in specific_terms}\n",
    "        if year not in term_frequencies:\n",
    "            term_frequencies[year] = Counter()\n",
    "        term_frequencies[year].update(frequencies)\n",
    "    return term_frequencies, total_word_counts\n",
    "\n",
    "# Function to prepare data for saving\n",
    "def prepare_data_for_saving(term_frequencies, total_word_counts):\n",
    "    plot_data = []\n",
    "    for year, frequencies in term_frequencies.items():\n",
    "        total_words = total_word_counts[year]\n",
    "        for term, frequency in frequencies.items():\n",
    "            relative_frequency = (frequency / total_words) * 1e6 if total_words > 0 else 0\n",
    "            plot_data.append({\n",
    "                'year': year, \n",
    "                'term': term, \n",
    "                'absolute_frequency': frequency, \n",
    "                'relative_frequency_per_million': relative_frequency\n",
    "            })\n",
    "    return pd.DataFrame(plot_data)\n",
    "\n",
    "# Create a filename from the terms in the specific_terms list\n",
    "terms_concatenated = '_'.join(specific_terms)\n",
    "\n",
    "# Dictionaries to hold cumulative term frequencies and total word counts across the file\n",
    "cumulative_frequencies = {}\n",
    "cumulative_word_counts = {}\n",
    "\n",
    "# Chunk size for reading the CSV file\n",
    "chunk_size = 100000\n",
    "\n",
    "file_path = os.path.join(directory_path, corpus)\n",
    "print(f\"Processing file: {file_path}\")\n",
    "\n",
    "# Read the file in chunks\n",
    "for chunk in pd.read_csv(file_path, delimiter='\\t', chunksize=chunk_size):\n",
    "    term_frequencies, total_word_counts = calculate_frequencies(chunk, specific_terms)\n",
    "    \n",
    "    # Add to cumulative frequencies and word counts\n",
    "    for year, frequencies in term_frequencies.items():\n",
    "        if year not in cumulative_frequencies:\n",
    "            cumulative_frequencies[year] = Counter()\n",
    "            cumulative_word_counts[year] = 0\n",
    "        cumulative_frequencies[year].update(frequencies)\n",
    "        cumulative_word_counts[year] += total_word_counts[year]\n",
    "\n",
    "# Convert cumulative frequencies to DataFrame for saving\n",
    "print(\"Converting cumulative frequencies to DataFrame for saving...\")\n",
    "cumulative_plot_df = prepare_data_for_saving(cumulative_frequencies, cumulative_word_counts)\n",
    "cumulative_plot_df['year'] = cumulative_plot_df['year'].astype(int)\n",
    "cumulative_plot_df = cumulative_plot_df.sort_values(by=['year'])\n",
    "\n",
    "# Save the results into a table (TSV)\n",
    "occurrences_output_filename = f'{terms_concatenated}_Occurrences.tsv'\n",
    "cumulative_plot_df.to_csv(os.path.join(outputDir, occurrences_output_filename), sep='\\t', index=False)\n",
    "print(f\"Occurrences table saved to {os.path.join(outputDir, occurrences_output_filename)}\")\n",
    "\n",
    "print(\"First cell finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: artensterben_bienensterben_insektensterben_waldsterben_klimawandel_Occurrences.tsv\n",
      "Absolute Frequency Visualization saved to e:\\Frequency_Timelines\\out\\html\\artensterben_bienensterben_insektensterben_waldsterben_klimawandel_absolute_frequency_Cumulative.html\n",
      "Relative Frequency per Million Visualization saved to e:\\Frequency_Timelines\\out\\html\\artensterben_bienensterben_insektensterben_waldsterben_klimawandel_relative_frequency_per_million_Cumulative.html\n",
      "Processing file: artensterben_bienensterben_insektensterben_waldsterben_Occurrences.tsv\n",
      "Absolute Frequency Visualization saved to e:\\Frequency_Timelines\\out\\html\\artensterben_bienensterben_insektensterben_waldsterben_absolute_frequency_Cumulative.html\n",
      "Relative Frequency per Million Visualization saved to e:\\Frequency_Timelines\\out\\html\\artensterben_bienensterben_insektensterben_waldsterben_relative_frequency_per_million_Cumulative.html\n",
      "Processing file: waldsterben_waldschädenbaumsterben_fichtensterben_tannensterben_Occurrences.tsv\n",
      "Absolute Frequency Visualization saved to e:\\Frequency_Timelines\\out\\html\\waldsterben_waldschädenbaumsterben_fichtensterben_tannensterben_absolute_frequency_Cumulative.html\n",
      "Relative Frequency per Million Visualization saved to e:\\Frequency_Timelines\\out\\html\\waldsterben_waldschädenbaumsterben_fichtensterben_tannensterben_relative_frequency_per_million_Cumulative.html\n",
      "Processing file: waldsterben_waldschäden_baumsterben_fichtensterben_tannensterben_Occurrences.tsv\n",
      "Absolute Frequency Visualization saved to e:\\Frequency_Timelines\\out\\html\\waldsterben_waldschäden_baumsterben_fichtensterben_tannensterben_absolute_frequency_Cumulative.html\n",
      "Relative Frequency per Million Visualization saved to e:\\Frequency_Timelines\\out\\html\\waldsterben_waldschäden_baumsterben_fichtensterben_tannensterben_relative_frequency_per_million_Cumulative.html\n",
      "Processing file: waldsterben_waldschäden_baumsterben_Occurrences.tsv\n",
      "Absolute Frequency Visualization saved to e:\\Frequency_Timelines\\out\\html\\waldsterben_waldschäden_baumsterben_absolute_frequency_Cumulative.html\n",
      "Relative Frequency per Million Visualization saved to e:\\Frequency_Timelines\\out\\html\\waldsterben_waldschäden_baumsterben_relative_frequency_per_million_Cumulative.html\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px  # Importing for access to color schemes\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "tsv_output_dir = cwd + '/out/csv/'  # Updated to use .tsv files\n",
    "html_output_dir = cwd + '/out/html/'\n",
    "\n",
    "# Get the list of all files in the tsv_output_dir that match \"Occurrences.tsv\"\n",
    "files = [f for f in os.listdir(tsv_output_dir) if \"Occurrences.tsv\" in f]\n",
    "\n",
    "if files:\n",
    "    for occurrences_output_filename in files:\n",
    "        print(f\"Processing file: {occurrences_output_filename}\")\n",
    "        cumulative_plot_df = pd.read_csv(os.path.join(tsv_output_dir, occurrences_output_filename), sep='\\t')\n",
    "\n",
    "        # Restrict the data to the range 1990-2020\n",
    "        cumulative_plot_df = cumulative_plot_df[(cumulative_plot_df['year'] >= 1990) & (cumulative_plot_df['year'] <= 2020)]\n",
    "\n",
    "        # Use a Plotly built-in color scheme\n",
    "        color_sequence = px.colors.qualitative.D3 \n",
    "\n",
    "        # Create the plot with custom styling for both absolute and relative frequencies\n",
    "        for freq_type, y_label, y_column in [('absolute_frequency', 'Absolute Frequency', 'absolute_frequency'), \n",
    "                                             ('relative_frequency_per_million', 'Relative Frequency per Million', 'relative_frequency_per_million')]:\n",
    "            \n",
    "            labels = cumulative_plot_df['term'].unique()\n",
    "            mode_size = [8, 8, 8, 8]\n",
    "            line_size = [2, 2, 2, 2]\n",
    "\n",
    "            fig = go.Figure()\n",
    "\n",
    "            for i, term in enumerate(labels):\n",
    "                term_data = cumulative_plot_df[cumulative_plot_df['term'] == term]\n",
    "                x_data = term_data['year']\n",
    "                y_data = term_data[y_column]\n",
    "\n",
    "                # Add the line trace for the term, using the color sequence\n",
    "                fig.add_trace(go.Scatter(x=x_data, y=y_data, mode='lines',\n",
    "                    name=term.capitalize(),\n",
    "                    line=dict(color=color_sequence[i % len(color_sequence)], width=line_size[i % len(line_size)]),\n",
    "                    marker=dict(size=2),\n",
    "                    connectgaps=True,\n",
    "                ))\n",
    "\n",
    "                # Add markers for the endpoints\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=[x_data.iloc[0], x_data.iloc[-1]],\n",
    "                    y=[y_data.iloc[0], y_data.iloc[-1]],\n",
    "                    mode='markers',\n",
    "                    marker=dict(color=color_sequence[i % len(color_sequence)], size=mode_size[i % len(mode_size)])\n",
    "                ))\n",
    "\n",
    "            # Custom layout styling\n",
    "            fig.update_layout(\n",
    "                width=1000,\n",
    "                xaxis=dict(\n",
    "                    showline=True,\n",
    "                    showgrid=True,\n",
    "                    gridcolor='rgba(200, 200, 200, 0.3)',\n",
    "                    showticklabels=True,\n",
    "                    linecolor='rgb(204, 204, 204)',\n",
    "                    linewidth=2,\n",
    "                    ticks='outside',\n",
    "                    tickmode='array',  # Custom tick mode\n",
    "                    tickvals=[i for i in range(1990, 2021, 2)], \n",
    "                    tickfont=dict(\n",
    "                        family='Arial',\n",
    "                        size=12,\n",
    "                        color='rgb(82, 82, 82)',\n",
    "                    ),\n",
    "                ),\n",
    "                yaxis=dict(\n",
    "                    showgrid=True,\n",
    "                    gridcolor='rgba(200, 200, 200, 0.3)', \n",
    "                    zeroline=False,\n",
    "                    showline=False,\n",
    "                    showticklabels=True,\n",
    "                    tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)'),\n",
    "                ),\n",
    "                autosize=False,\n",
    "                margin=dict(\n",
    "                    autoexpand=False,\n",
    "                    l=100,\n",
    "                    r=150,\n",
    "                    t=110,\n",
    "                ),\n",
    "                plot_bgcolor='white',\n",
    "                showlegend=False  # Remove the legend\n",
    "            )\n",
    "\n",
    "            # Add annotations for the right-side labels, slightly moved to the right\n",
    "            annotations = []\n",
    "            for i, term in enumerate(labels):\n",
    "                term_data = cumulative_plot_df[cumulative_plot_df['term'] == term]\n",
    "                y_trace = term_data[y_column].values\n",
    "                # Labeling the right side of the plot (end of the line)\n",
    "                annotations.append(dict(xref='paper', x=1.01, y=y_trace[-1],  # Adjusted to move labels slightly to the right\n",
    "                                        xanchor='left', yanchor='middle',\n",
    "                                        text=term.capitalize(),\n",
    "                                        font=dict(family='Arial', size=16, color=color_sequence[i % len(color_sequence)]),\n",
    "                                        showarrow=False))\n",
    "\n",
    "            # Update layout with annotations\n",
    "            fig.update_layout(annotations=annotations)\n",
    "\n",
    "            # Save the plot as an HTML file\n",
    "            output_filename = occurrences_output_filename.replace('Occurrences.tsv', f'{freq_type}_Cumulative.html')\n",
    "            fig.write_html(file=os.path.join(html_output_dir, output_filename), include_plotlyjs=True)\n",
    "            print(f\"{y_label} Visualization saved to {os.path.join(html_output_dir, output_filename)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No 'Occurrences.tsv' files found in the specified directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
